---
title: "Sparse Contrastive Principal Component Analysis for Exploring High-Dimensional Biological Data"
subtitle: ""
author: "Philippe Boileau"
institute: Graduate Group in Biostatistics <br> University of California, Berkeley <br> In Collaboration with Nima Hejazi and Sandrine Dudoit
date: ""
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, metropolis, metropolis-fonts, font-size.css]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: ["addons/macros.js"]
---
class: middle, center, inverse

# Motivation

```{r load_refs, include=FALSE, cache=FALSE}
library(RefManageR)
BibOptions(
  check.entries = FALSE,
  bib.style = "authoryear",
  cite.style = "authoryear",
  style = "markdown",
  hyperlink = TRUE
)
my_bib <- ReadBib("references.bib", check = FALSE)
```

???

First, we'll briefly motivate our dimensionality reduction method.

---
class: left, top

# Motivation

A common problem in the analysis of high-dimensional biological data is the
teasing out of biological signal from technical noise, i.e. the removal of
*unwanted* variation. 

.center[![:scale 60%](figures/batch_effect_example.png)]

???

As you well know, a common issues that arises in the analysis of high-dimensional biological data is the teasing out of meaningful signal from technical noise.
In this toy example over here, the first two principal components of some data
are presented. There are two distinct biological groups, one green, one purple, and
each group is split by a batch effect, where each observations' batch is denoted
by it's shape.

Before we analyze this data, we'll have to remove this batch effect. Now there
are plenty of options to do this, but many of them make some restrictive assumptions
about how this data has been generated. Instead, we'd like to remove this unwanted
variation without needing to make untenable assumptions. This is where our
scPCA method can be of use.

---
class: middle, center, inverse

# Background

???

Now, scPCA is simply the combination of two different dimensionality reduction
methods: contrastive PCA (or cPCA for short), and sparse PCA (referred to as SPCA).
I'll briefly introduce these two methods before going any further.

---
class: left, top

# cPCA

Given a **target** dataset containing both *wanted* and *unwanted* variation,
and a **background** dataset containing only *unwanted* variation, cPCA produces
a low-dimensional embedding of the target data reflecting (a portion of)
the variation not found in the background data.

.center[
![](figures/cPCA.png)

*Figure 1, `r Citet(my_bib, "Abid2018")`*
]

???

cPCA provides its users with a novel approach for the removal of unwanted variation
by taking advantage of experimental design which generated the data. That is,
given a dataset which contains wanted and unwanted variation, which we refer
to as the target, and a dataset containing only the unwanted variation, referred
to as the background data, we can deflate the unwanted variation in the target
data.

A sketch of how this is accomplished is nicely summarized in the first figure
of Abid and others, the developers of this method. First, the sample covariance
matrices of the target and background data are computed. Next, we take the
difference of the target's and background's sample covariance matrices, scaling
the background's matrix by a hyperparameter, known as the contrastive parameter.
We then get the eigendecomposition of the resulting symmetric matrix, the
contrastive matrix, and use the decomposition's loadings vectors to rotate the
target data. By appropriately tuning the contrastive parameter, we're effectively
able to remove the noise in the target.

In this example over here, we see that the first principal component corresponds
the red, dotted line. This axis of variation isn't all that interesting; it
won't separate the groups contained in the target data. However, the leading
cPCA will be able to deflate this un-interesting variation.

---
class: left, top

# Drawbacks

1. Like PCA in high dimensions, interpreting cPCA's loadings is difficult.
2. The contrastive parameter, $\alpha$, is selected visually.

.center[
![](figures/machine_learning.png)

*[xkcd.com/1838/](https://xkcd.com/1838/)*
]

???

Although, this is an interesting approach to the removal of unwanted variation,
there are a few issues. Namely, since this method is essentially PCA but applied
to a different covariance matrix, it suffers from the same issues as PCA does in
high dimensions. In particular, interpreting cPCA's loadings vectors, the vectors
that define the linear combinations of each contrastive principal component, is
quite difficult, and they are potentially unstable. Even worse, selecting the
appropriate contrastive parameter is accomplished through visual inspection. All
analyses are subjective to some degree, but this takes that to another level.

---
class: left, top

# SPCA

SPCA `r Citep(my_bib, "Zou2006")` provides interpretable loadings that are more
stable in high dimensions. How is this accomplished?

???

Based on these drawbacks, you probably see where I'm going with SPCA. For those
of you who aren't familiar with SPCA, it's a method originally developed by
Zou et al that increases the interpretability and stability of PCA in high
dimensional settings. 

--

1. Re-frame PCA as a regression problem: PCA produces a linear manifold
approximation of the data.
2. Extend elasticnet regression to this regression problem. That is, include
$\ell_1$ and $\ell_2$ penalty terms to induce sparsity in loadings vectors.

???

This is accomplished by first re-framing PCA as a linear regression problem, and
then including L1 and L2 penalties in the objective function to induce sparsity
in the loadings vectors. This is analogous to an elasticnet regression. And
just like in classic elasticnet, lasso, and ridge regressions, the penalty
hyperparamters have to be selected somehow.

--


This second step isn't as straightforward as in the typical regression setting,
but it is possible to do it efficiently `r Citep(my_bib, "Erichson_2020")`.

???

Now, this process isn't exactly as easy as I've let on, but there are some
very efficient numerical methods to solve this problem. One that we employ in our
package is that of Erichson and others.

---
class: middle, center, inverse

# Sparse Contrastive Principal Component Analysis (scPCA)

???

With the background done, we can finally talk about scPCA.

---
class: left, top

# scPCA

scPCA consists of applying SPCA on the contrastive covariance matrix of
target and background datasets. We get the best of both of cPCA and SPCA:

* Unwanted variation is removed from the target dataset
* scPCA's loadings are interpretable
* scPCA's loadings are stable

???

As I mentioned before, scPCA is essentially a combination of the cPCA and SPCA,
requiring only a few minor modifications required to make these methods well
together.

The result is a sparse version of cPCA. That is, scPCA's are now
interpretable and stable in the high-dimensional settings that we so often
encounter when analyzing the data output by high-throughput experiments.

--

But what about the hyperparameters?

???

However, we're note completely done. We still need to come up with a way to
select scPCA's hyperparameters: the contrastive parameters, and an L1 penalty
term that controls the levels of sparsity.

---
class: left, top

# Hyperparameter Tuning

There are two main hyperparameters in the scPCA algorithm:
+ The constrastive parameter $\alpha$
+ The $\ell_1$-penalty parameter $\lambda_1$ 

Hyperparameters are selected via a clustering-based grid-search method:

1. A grid of hyperparameters is specified
2. scPCA is performed for each pair of hyperparameters
3. A clustering algorithm is applied to the resulting low-dimensional embedding
4. Each embedding's quality is evaluated via the average silhouette width

The pair of hyperparameters that results in the largest average silhouette width
are identified as *optimal*.

A cross-validated variation also exists if finding non-generalizable patterns
in the data is a concern.

???

Now, selecting an optimal set of hyperparameters isn't an easy task in this kind
of unsupervised setting since we don't have access to ground truth. However, if
we're willing to make a few assumptions about the data, then we can come up with
a reasonable heuristic for selecting a set of reasonable parametes automatically.

Our procedure goes as follows: first, we specify a grid of hyperparameters, then
we perform scPCA on our data for each combination of the L1 penalty and constrative paramerter. Next, we apply a clustering algorithm of our choice on the low-dimensional
embedding, and then evaluate the separation of the clusters based on some clustering
metric. We found that the average silhouette method performed quite well for this task.

Now, this procedure only works if we expect to find distinct clusers in the
data that we're analyzing. Thankfully is often the case anyways.
Additionally, since this is is only a heuristic, we made a cross-validated version
to decrease the chances of finding non-generalizable patterns in our
low-dimensional embedding. It's also worth mentioning that this heuristic can
be generalized to other dimensionality reduction procedures.

---
class: left, top

# Implementation

The `scPCA` R package, released in Bioconductor 3.10, implements:
1. scPCA
2. cPCA with automated hyperparameter tuning
3. cross-validated scPCA and cPCA
4. cPCA with visual hyperparameter tuning

???

Now that you're familiar with the method, we can talk briefly about its
implementation. scPCA has been implemented in the scPCA package, which was first
released in Bioconductor 3.10. This package also implements two flavours of
cPCA: cPCA with automated contrastive parameter tuning using the previously
discussed heuristic, as well as the original method with visual tuning.

--

### Relevant details:

+ The hyperparameter tuning framework is embarrassingly parallel. Methods 1, 2, and 3 possess parallelized counterparts, making use of the `BiocParallel` infrastructure.
+ The `scPCA` package integrates fully with the `SingleCellExperiment` container class, allowing for these methods' seamless inclusion in scRNA-seq analysis pipelines. 
+ The scPCA method is applied to a contrastive covariance matrix; its running time is impacted by the number of features, not observations.

???

There are also some relevant details that worth mentioning: First, the hyperparameter
tuning framework is embarrasingly parallel, and so all the package's methods that
use it also possess parallel counterparts. These parallel versions rely on the
BiocParallel infrastucture. Second, for those of you who might want to apply this
method to some single cell RNA-seq data, the scPCA package integrates fully
with the SingleCellExperiment container class. And, finally, the contrastive
dimension reduction methods operate over a covariance matrix like structure. That
means the computational complexity grows with the number of features, and not the
number of observations. This can be a curse in some cases, but, if you're working
with a large dataset, you'll find that these methods are significantly faster
than other popular dimension reduction methods.

---
class: middle, center, inverse

# Example

???

So, with that said, let's take a look at scPCA in action.

---
class: left, top

# Simulated scRNA-seq Data

300 cells and 500 genes were simulated using the `Splat` framework of
`r Citet(my_bib, "Zappia2017")`, and split into 3 equally sized groups.

.center[![:scale 60%](figures/full_sim_data.png)]

A number of leading dimensionality reduction methods were applied to the target
data to determine if they could disentangle the biological signal from the
batch effect.

???

We'll apply scPCA to a simulated, toy, single cell RNA-seq dataset where the
ground truth is known. This small dataset, simulated with the Splat framework,
consists of 300 cells, and 500 genes. These Cells are split into three groups
of roughly equal size. The turquoise group corresponds to an uninteresting control 
group, and the other two groups in green and purple correspond to the observations
that we're actually interested in differentiating. All three of these simulated
groups are split by the same batch effect.

In the next slide, we apply a set of general dimension reduction methods to see
if we can remove this batch from the target data.

---
class: left, top

# Simulated scRNA-seq Data (Cont.)

.center[![](figures/sim_results.png)]

???

Unsurprisingly, t-SNE and UMAP are unable to remove the batch effect; how could
they? This is something of a sanity check. Next, let's look at cPCA. At first
glance, the batch effect seems to have been removed. However, if we look
closely, you'll notice that the observations are still stratified by batch along
the second contrastive principal component. Now turning, our attention to the
scPCA embedding, we find that the batch effect has been completely removed. The
sparsification had the effect of removing the remaining amount of technical noise
that the contrastive procedure couldn't. 

We also wanted to compare scPCA's results to a state of the art method dimension
reduction method for scRNA-seq: zinb-wave. On the bottom right pane, zinb-wave was
run as if the batch effect were a known and measured confounder, and it successfully
removed the batch effect - though not as well as scPCA did. On the bottem left,
zinb-wave was run as if the batch was an unknown condounder, and, as with t-SNE
and UMAP, was not able to remove the simulated technical noise.

---
class: inverse, center, middle

# Fin.

See our paper, Exploring High-Dimensional Biological Data with Sparse
Contrastive Principal Component Analysis, for more information.

???

And that's it. That's all I have time to show you today. If scPCA has picked your
interest, I encourage to checkout our paper, and to take a look at our packages
vignette. Thanks!

---
class: left, top

# References

```{r refs_1, echo = FALSE, results='asis'}
PrintBibliography(my_bib)
```
